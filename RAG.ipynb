{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia RAG with Chroma & FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.3;\">\n",
    "This project implements a Retrieval-Augmented Generation (RAG) system using data extracted from the Simple English Wikipedia. It leverages LangChain to orchestrate the process, Sentence Transformers for creating embeddings, uses ChromaDB and FAISS as vector stores for retrieval, and uses a LLM via LangChain for generating answers based on the retrieved context.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to the GitHub repository: https://github.com/jean-ferrer/WikiRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "from API_TOKEN import API_TOKEN\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import pickle\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_docs = []\n",
    "\n",
    "docs_path = Path(\"docs\")\n",
    "\n",
    "# Searches for all files in subfolders, regardless of the extension\n",
    "for file_path in docs_path.rglob(\"*\"):\n",
    "    if file_path.is_file(): # ignores folders\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    article = json.loads(line)\n",
    "                    text = article.get(\"text\", \"\").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "                    if text:\n",
    "                        wiki_docs.append(\n",
    "                            Document(\n",
    "                                page_content=text,\n",
    "                                metadata={\n",
    "                                    \"title\": article.get(\"title\", \"Unknown\"),\n",
    "                                    \"source_file\": str(file_path.relative_to(docs_path)) # example: \"AA/wiki_00\"\n",
    "                                }\n",
    "                            )\n",
    "                        )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "### Code to download all-MiniLM-L6-v2 ###\n",
    "\n",
    "# Destination path within your project\n",
    "local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Checks if the folder already exists and contains files\n",
    "if os.path.exists(local_model_path) and os.listdir(local_model_path):\n",
    "    print(f\"Model already exists at: {local_model_path}\")\n",
    "else:\n",
    "    # Downloads the model to the local Hugging Face cache\n",
    "    cached_model_path = snapshot_download(repo_id=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Creates the destination directory if it doesn't exist\n",
    "    os.makedirs(local_model_path, exist_ok=True)\n",
    "\n",
    "    # Copies all files from the cache to the project folder\n",
    "    for item in os.listdir(cached_model_path):\n",
    "        s = os.path.join(cached_model_path, item)\n",
    "        d = os.path.join(local_model_path, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "    print(f\"Model copied to: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jean\\AppData\\Local\\Temp\\ipykernel_5196\\3852959615.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=local_model_path)\n"
     ]
    }
   ],
   "source": [
    "# Embedding function using a light and efficient model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Chroma Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading existing vector store...\n",
      "\n",
      "‚úÖ Chroma vector database and retriever are ready.\n",
      "Retriever type: VectorStoreRetriever\n"
     ]
    }
   ],
   "source": [
    "# Define the Chroma directory\n",
    "CHROMA_DIR = \"chroma_db\"\n",
    "COLLECTION_NAME = \"SimpleWiki\"\n",
    "\n",
    "# Check if the directory already exists to avoid recreating the vector store\n",
    "if os.path.exists(CHROMA_DIR) and os.listdir(CHROMA_DIR):\n",
    "    print(\"üîÑ Loading existing vector store...\")\n",
    "    vector_db_chroma = Chroma(\n",
    "        persist_directory=CHROMA_DIR,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "else:\n",
    "    print(\"üÜï Creating new Chroma vector store and saving to disk...\")\n",
    "\n",
    "    # Initialize Chroma with embedding function and persist directory\n",
    "    vector_db_chroma = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=CHROMA_DIR,\n",
    "        collection_name=COLLECTION_NAME\n",
    "    )\n",
    "\n",
    "    # Separate texts and metadata\n",
    "    texts = [doc.page_content for doc in wiki_docs]\n",
    "    metadatas = [doc.metadata for doc in wiki_docs]\n",
    "\n",
    "    # Generate embeddings for Chroma\n",
    "    print(\"üìä Generating embeddings for Chroma...\")\n",
    "    embeddings_list_chroma = [embeddings.embed_query(text) for text in tqdm(texts, desc=\"üì• Embedding (Chroma)\")]\n",
    "    print(f\"‚úÖ Embeddings generated for {len(embeddings_list_chroma)} documents (Chroma).\")\n",
    "\n",
    "    # Create documents in Langchain format\n",
    "    documents = [Document(page_content=texts[i], metadata=metadatas[i]) for i in range(len(texts))]\n",
    "\n",
    "    # Add documents with their embeddings to the vector store\n",
    "    print(\"üì• Adding documents to the Chroma vector store...\")\n",
    "    for i in tqdm(range(len(documents)), desc=\"‚ûï Adding to Chroma\"):\n",
    "        vector_db_chroma.add_documents(documents=[documents[i]], embeddings=[embeddings_list_chroma[i]])\n",
    "\n",
    "# Convert the vector store to a retriever with MMR search\n",
    "chroma_retriever = vector_db_chroma.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Chroma vector database and retriever are ready.\")\n",
    "print(f\"Retriever type: {chroma_retriever.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract semantically relevant text with Chroma\n",
    "def extract_text_chroma(query: str, max_words_per_doc: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves snippets of text semantically relevant to the query using a Retriever,\n",
    "    with a word limit per document and removal of exact duplicates.\n",
    "    \"\"\"\n",
    "    results = chroma_retriever.invoke(query)\n",
    "    if not results:\n",
    "        return \"No matching information found.\"\n",
    "\n",
    "    seen = set()\n",
    "    cleaned_chunks = []\n",
    "\n",
    "    for doc in results:\n",
    "        content = doc.page_content.strip()\n",
    "        words = re.findall(r'\\S+', content)\n",
    "        trimmed = \" \".join(words[:max_words_per_doc])\n",
    "        normalized = re.sub(r\"\\s+\", \" \", trimmed).strip()\n",
    "\n",
    "        if normalized.lower() not in seen:\n",
    "            seen.add(normalized.lower())\n",
    "            cleaned_chunks.append(trimmed)\n",
    "\n",
    "    return \"\\n\\n\".join(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peach is a species of the \"Prunus persica\", and is a fruit tree of the rose family \"Rosaceae\". They grow in the warm regions of both the northern and southern hemispheres. Description. Peach blossoms are small to medium-sized. The tree is sometimes up to 6.5 m (21 feet) in height. When it is grown by people, the height it is usually kept between 3 and 4 m (10 and 13 feet) by pruning. Its leaves are green and pointy. They usually have glands that make a liquid to attract insects. Peaches are also called stone fruits because they have a shell of hard wood around their seed, called a stone or a pit. The skin of a peach is an orange or yellow color, and it is covered in small hairs called \"peach fuzz\". A peach without the fuzz is usually called a nectarine. The inside of a peach is a golden color. It tastes sweet and sticky. Because of this, peaches are often part of desserts. Symbolism. The peach first came from China. It has been grown from at least since 1000 B.C.E. In Chinese culture, the peach tree is considered to be the tree of life and\n",
      "\n",
      "In computer science, a tree is a graph data structure composed of items that have child items. Trees have an item called a root. No item has the root as a child. Trees may not have cycles. Items may contain a reference to their parent. An item is a leaf if it has no children. The height of an item is the length of the longest downward path to a leaf from that item. The height of the root is the height of the tree. The depth of an item is the length of the path to the tree's root, tree roots can be infinite.\n",
      "\n",
      "The aquatic tree frog or Bogert's aquatic tree frog (\"Sarcohyla crassa\") is a frog that lives in Mexico. It lives in cloud forests. Scientists have seen it between 1543 and 2300 meters above sea level in the Sierra Mixes Sierra Ju√°rez and once at 2652 in the Sierra Madre del Sur. References. &lt;templatestyles src=\"Reflist/styles.css\" /&gt; \"This about can be made longer. You can help Wikipedia by [ adding to it]\".\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the average height of a peach tree?'\n",
    "print(extract_text_chroma(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement FAISS Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading existing FAISS vector store...\n",
      "\n",
      "‚úÖ FAISS vector database and retriever are ready.\n",
      "Retriever type: VectorStoreRetriever\n"
     ]
    }
   ],
   "source": [
    "# Define the FAISS directory and filename\n",
    "FAISS_DIR = \"faiss_db\"\n",
    "FAISS_INDEX_FILE = \"faiss_index.pkl\"\n",
    "FAISS_PATH = Path(FAISS_DIR) / FAISS_INDEX_FILE\n",
    "\n",
    "# Initialize FAISS index\n",
    "embedding_dimension = len(embeddings.embed_query(\"hello world\"))\n",
    "\n",
    "# Check if the directory already exists to avoid recreating the vector store\n",
    "if os.path.exists(FAISS_PATH):\n",
    "    print(\"üîÑ Loading existing FAISS vector store...\")\n",
    "    # Load the entire vector_db_faiss object (which includes index, docstore, etc.)\n",
    "    with open(FAISS_PATH, \"rb\") as f:\n",
    "        vector_db_faiss = pickle.load(f)\n",
    "else:\n",
    "    print(\"üÜï Creating new FAISS vector store and saving to disk...\")\n",
    "\n",
    "    # Create FAISS directly from documents\n",
    "    print(\"üìä Generating embeddings and building FAISS from documents...\")\n",
    "    vector_db_faiss = FAISS.from_documents(wiki_docs, embeddings) # This step takes a while, be patient\n",
    "    print(f\"‚úÖ FAISS vector store created and populated with {len(wiki_docs)} documents.\")\n",
    "\n",
    "    # Save FAISS vector store to disk\n",
    "    FAISS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(FAISS_PATH, \"wb\") as f:\n",
    "        pickle.dump(vector_db_faiss, f) # Pickle the entire vector_db_faiss object\n",
    "    print(f\"üíæ FAISS vector store saved to: {FAISS_PATH}\")\n",
    "\n",
    "# Convert the FAISS vector store to a retriever\n",
    "faiss_retriever = vector_db_faiss.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"\\n‚úÖ FAISS vector database and retriever are ready.\")\n",
    "print(f\"Retriever type: {faiss_retriever.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract semantically relevant text with FAISS\n",
    "def extract_text_faiss(query: str, retriever=faiss_retriever, max_words_per_doc: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves snippets of text semantically relevant to the query using a Retriever,\n",
    "    with a word limit per document and removal of exact duplicates.\n",
    "    \"\"\"\n",
    "    # Use the retriever's invoke method to get the relevant documents\n",
    "    results: list[Document] = retriever.invoke(query)\n",
    "\n",
    "    if not results:\n",
    "        return \"No matching information found.\"\n",
    "\n",
    "    seen = set()\n",
    "    cleaned_chunks = []\n",
    "\n",
    "    for doc in results:\n",
    "        # Ensure the document content is a string\n",
    "        content = str(doc.page_content).strip()\n",
    "\n",
    "        # Split content into words and take the first max_words_per_doc\n",
    "        words = re.findall(r'\\S+', content)\n",
    "        trimmed = \" \".join(words[:max_words_per_doc])\n",
    "\n",
    "        # Normalize whitespace and convert to lowercase for duplicate checking\n",
    "        normalized = re.sub(r\"\\s+\", \" \", trimmed).strip().lower()\n",
    "\n",
    "        # Add the trimmed chunk if its normalized version hasn't been seen before\n",
    "        if normalized not in seen:\n",
    "            seen.add(normalized)\n",
    "            # Append the original trimmed version (not lowercase)\n",
    "            cleaned_chunks.append(trimmed)\n",
    "\n",
    "    if not cleaned_chunks:\n",
    "         return \"No relevant information found after processing.\"\n",
    "\n",
    "    return \"\\n\\n\".join(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peach is a species of the \"Prunus persica\", and is a fruit tree of the rose family \"Rosaceae\". They grow in the warm regions of both the northern and southern hemispheres. Description. Peach blossoms are small to medium-sized. The tree is sometimes up to 6.5 m (21 feet) in height. When it is grown by people, the height it is usually kept between 3 and 4 m (10 and 13 feet) by pruning. Its leaves are green and pointy. They usually have glands that make a liquid to attract insects. Peaches are also called stone fruits because they have a shell of hard wood around their seed, called a stone or a pit. The skin of a peach is an orange or yellow color, and it is covered in small hairs called \"peach fuzz\". A peach without the fuzz is usually called a nectarine. The inside of a peach is a golden color. It tastes sweet and sticky. Because of this, peaches are often part of desserts. Symbolism. The peach first came from China. It has been grown from at least since 1000 B.C.E. In Chinese culture, the peach tree is considered to be the tree of life and\n",
      "\n",
      "Peach is a color that represents the color of the peach fruit. Peach paint can be made by mixing orange paint and white paint. The first written use of \"peach\" as a color name in English was in 1588. Where the word \"peach\" came from. The word \"peach\" comes from the Middle English \"peche\", from Middle French, in turn from Latin \"persica\", i.e., \"the fruit from Persia\". In fact, the origin of the peach fruit was from China. References. \"This can be made longer. You can help Wikipedia by [ adding to it]\".&lt;/div &gt;\n",
      "\n",
      "Peach Orchard is a city of Clay County in the state of Arkansas in the United States. References. &lt;templatestyles src=\"Reflist/styles.css\" /&gt; \"This about a can be made longer. You can help Wikipedia by [ adding to it]\".\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the average height of a peach tree?'\n",
    "print(extract_text_faiss(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available models...\n",
      "- Name: models/gemini-1.0-pro-vision-latest\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-pro-vision\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro-latest\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro-001\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro-002\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-pro\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-latest\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-001\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-001-tuning\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-002\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b\n",
      "  Supported Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-001\n",
      "  Supported Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-latest\n",
      "  Supported Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-exp-0827\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-1.5-flash-8b-exp-0924\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-2.5-pro-exp-03-25\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.5-pro-preview-03-25\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.5-flash-preview-04-17\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-exp\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-001\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-exp-image-generation\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite-001\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-lite-preview\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-pro-exp\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-pro-exp-02-05\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-exp-1206\n",
      "  Supported Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-thinking-exp\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/learnlm-1.5-pro-experimental\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/learnlm-2.0-flash-experimental\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemma-3-1b-it\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemma-3-4b-it\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemma-3-12b-it\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "- Name: models/gemma-3-27b-it\n",
      "  Supported Methods: ['generateContent', 'countTokens']\n",
      "--------------------\n",
      "Complete listing.\n"
     ]
    }
   ],
   "source": [
    "# Checking Google's available LLM models\n",
    "\n",
    "print(\"Listing available models...\")\n",
    "genai.configure(api_key=API_TOKEN)\n",
    "for model in genai.list_models():\n",
    "  # Checks if the model supports the generateContent operation (for chat/text)\n",
    "  if 'generateContent' in model.supported_generation_methods:\n",
    "    print(f\"- Name: {model.name}\")\n",
    "    print(f\"  Supported Methods: {model.supported_generation_methods}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print(\"Complete listing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define safety settings (\"guard rails\")\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the Google Gemini Flash language model for chat\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=API_TOKEN, temperature=0.1) # Sets a low temperature for more direct and deterministic responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This template instructs the LLM on how to use the context.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful and informative assistant.\n",
    "      Use ONLY the following provided context to answer the question.\n",
    "      If you cannot find the answer in the provided context, simply say that you do not have enough information.\n",
    "      Do not use your prior knowledge.\n",
    "      Ensure the answer is clear and concise.\"\"\"),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {input}\")\n",
    "])\n",
    "\n",
    "# First, we create a chain that \"stuffs\" all the retrieved documents into the {context} slot of the prompt.\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Defining the two retrievers\n",
    "rag_chain_chroma = create_retrieval_chain(chroma_retriever, document_chain)\n",
    "rag_chain_faiss = create_retrieval_chain(faiss_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the question\n",
    "query_rag = \"What is the average height of a peach tree?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Asking: What is the average height of a peach tree?\n",
      "\n",
      "--- LLM Response ---\n",
      "When grown by people, peach trees are usually kept between 3 and 4 m (10 and 13 feet) by pruning.\n"
     ]
    }
   ],
   "source": [
    "# Print the question being asked\n",
    "print(f\"\\n‚ùì Asking: {query_rag}\")\n",
    "\n",
    "# The result will contain the answer ('answer') and the source documents ('context') retrieved from Chroma\n",
    "response = rag_chain_chroma.invoke({\"input\": query_rag})\n",
    "\n",
    "# Print the LLM's response based on the retrieved context\n",
    "print(\"\\n--- LLM Response ---\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Source Documents (Context) ---\n",
      "- Peach (Source: AA\\wiki_12)\n",
      "The peach is a species of the \"Prunus persica\", and is a fruit tree of the rose family \"Rosaceae\". They grow in the warm regions of both the northern and southern hemispheres. Description. Peach blossoms are small to medium-sized. The tree is sometimes up to 6.5 m (21 feet) in height. When it is grown by people, the height it is usually kept between 3 and 4 m (10 and 13 feet) by pruning. Its leaves are green and pointy. They usually have glands that make a liquid to attract insects. Peaches are ...\n",
      "- Tree (data structure) (Source: AB\\wiki_88)\n",
      "In computer science, a tree is a graph data structure composed of items that have child items. Trees have an item called a root. No item has the root as a child. Trees may not have cycles. Items may contain a reference to their parent. An item is a leaf if it has no children. The height of an item is the length of the longest downward path to a leaf from that item. The height of the root is the height of the tree. The depth of an item is the length of the path to the tree's root, tree roots can ...\n",
      "- Sarcohyla crassa (Source: AC\\wiki_07)\n",
      "The aquatic tree frog or Bogert's aquatic tree frog (\"Sarcohyla crassa\") is a frog that lives in Mexico. It lives in cloud forests. Scientists have seen it between 1543 and 2300 meters above sea level in the Sierra Mixes Sierra Ju√°rez and once at 2652 in the Sierra Madre del Sur. References. &lt;templatestyles src=\"Reflist/styles.css\" /&gt;  \"This about can be made longer. You can help Wikipedia by [ adding to it]\"....\n"
     ]
    }
   ],
   "source": [
    "# Print the source documents that were used\n",
    "print(\"\\n--- Source Documents (Context) ---\")\n",
    "for doc in response[\"context\"]:\n",
    "    # Print document metadata (title and source file) if available\n",
    "    print(f\"- {doc.metadata.get('title', 'Untitled')} (Source: {doc.metadata.get('source_file', 'N/A')})\")\n",
    "    # Print the first 500 characters of the document content\n",
    "    print(doc.page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Asking: What is the average height of a peach tree?\n",
      "\n",
      "--- LLM Response ---\n",
      "When grown by people, peach trees are usually kept between 3 and 4 m (10 and 13 feet) by pruning.\n"
     ]
    }
   ],
   "source": [
    "# Print the question being asked\n",
    "print(f\"\\n‚ùì Asking: {query_rag}\")\n",
    "\n",
    "# The result will contain the answer ('answer') and the source documents ('context') retrieved from FAISS\n",
    "response = rag_chain_faiss.invoke({\"input\": query_rag})\n",
    "\n",
    "# Print the LLM's response based on the retrieved context\n",
    "print(\"\\n--- LLM Response ---\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Source Documents (Context) ---\n",
      "- Peach (Source: AA\\wiki_12)\n",
      "The peach is a species of the \"Prunus persica\", and is a fruit tree of the rose family \"Rosaceae\". They grow in the warm regions of both the northern and southern hemispheres. Description. Peach blossoms are small to medium-sized. The tree is sometimes up to 6.5 m (21 feet) in height. When it is grown by people, the height it is usually kept between 3 and 4 m (10 and 13 feet) by pruning. Its leaves are green and pointy. They usually have glands that make a liquid to attract insects. Peaches are ...\n",
      "- Peach (color) (Source: AA\\wiki_55)\n",
      "Peach is a color that represents the color of the peach fruit.  Peach paint can be made by mixing orange paint and white paint.  The first written use of \"peach\" as a color name in English was in 1588. Where the word \"peach\" came from. The word \"peach\" comes from the Middle English \"peche\", from Middle French, in turn from Latin \"persica\", i.e., \"the fruit from Persia\". In fact, the origin of the peach fruit was from China. References.  \"This can be made longer. You can help Wikipedia by [ addin...\n",
      "- Peach Orchard, Arkansas (Source: AA\\wiki_58)\n",
      "Peach Orchard is a city of Clay County in the state of Arkansas in the United States. References. &lt;templatestyles src=\"Reflist/styles.css\" /&gt;  \"This about a¬† can be made longer. You can help Wikipedia by [ adding to it]\"....\n"
     ]
    }
   ],
   "source": [
    "# Print the source documents that were used\n",
    "print(\"\\n--- Source Documents (Context) ---\")\n",
    "for doc in response[\"context\"]:\n",
    "    # Print document metadata (title and source file) if available\n",
    "    print(f\"- {doc.metadata.get('title', 'Untitled')} (Source: {doc.metadata.get('source_file', 'N/A')})\")\n",
    "    # Print the first 500 characters of the document content\n",
    "    print(doc.page_content[:500] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
